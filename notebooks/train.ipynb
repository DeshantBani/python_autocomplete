{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Github](https://img.shields.io/github/stars/lab-ml/python_autocomplete?style=social)](https://github.com/lab-ml/python_autocomplete)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lab-ml/python_autocomplete/blob/master/notebooks/train.ipynb)\n",
    "\n",
    "# Train a character level autoregressive model on Python source code\n",
    "\n",
    "This notebook will download repositories linked from [Awesome PyTorch List](https://github.com/bharathgs/Awesome-pytorch-list/) and train a character level model.\n",
    "\n",
    "[Evaluation notebook](https://github.com/lab-ml/python_autocomplete/blob/master/notebooks/evaluate.ipynb) evaluates the trained model on some samples. [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lab-ml/python_autocomplete/blob/master/notebooks/evaluate.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_TGiFpndux7n",
    "outputId": "c9c7028d-bdb4-45b1-d3a1-15d6d8d50104"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install labml labml_python_autocomplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "A45AlnGKvWxQ"
   },
   "outputs": [],
   "source": [
    "from python_autocomplete import create_dataset\n",
    "from labml.logger import inspect\n",
    "from labml import monit, lab\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset\n",
    "\n",
    "We will step-by-step\n",
    "\n",
    "1. download `readme` file from Awesome PyTorch List\n",
    "2. pick github links in it\n",
    "3. download those repositories\n",
    "4. remove non python files\n",
    "5. merge all python files into a training/validation text\n",
    "\n",
    "The code for this is in [`create_dataset.py`](https://github.com/lab-ml/python_autocomplete/blob/master/python_autocomplete/create_dataset.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fJHF_qsvu5zz"
   },
   "outputs": [],
   "source": [
    "create_dataset.create_folders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LKEDKTI-Fes"
   },
   "source": [
    "Get the list of repositories from [Awesome-PyTorch-list](https://github.com/bharathgs/Awesome-pytorch-list/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "9WqEth0cOD8e",
    "outputId": "b94874d3-b826-40a3-a6f2-e095bd473302"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\"><span style=\"color: #60C6C8\">  0: </span><strong>('pytorch', 'text')</strong>\n",
       "<span style=\"color: #60C6C8\">  1: </span><strong>('IBM', 'pytorch-seq2seq')</strong>\n",
       "<span style=\"color: #60C6C8\">  2: </span><strong>('Sandeep42', 'anuvada')</strong>\n",
       "<span style=\"color: #60C6C8\">  3: </span><strong>('pytorch', 'audio')</strong>\n",
       "<span style=\"color: #60C6C8\">  4: </span><strong>('facebookresearch', 'loop')</strong>\n",
       "<span style=\"color: #60C6C8\">  5: </span><strong>('facebookresearch', 'fairseq-py')</strong>\n",
       "<span style=\"color: #60C6C8\">  6: </span><strong>('awni', 'speech')</strong>\n",
       "<span style=\"color: #60C6C8\">  7: </span><strong>('OpenNMT', 'OpenNMT-py')</strong>\n",
       "<span style=\"color: #60C6C8\">  8: </span><strong>('huggingface', 'neuralcoref')</strong>\n",
       "<span style=\"color: #60C6C8\">  9: </span><strong>('NVIDIA', 'sentiment-discovery')</strong>\n",
       "<span style=\"color: #208FFB\">...</span>\n",
       "Total <span style=\"color: #208FFB\">665</span> item(s)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_dataset.get_awesome_pytorch_readme()\n",
    "repos = create_dataset.get_repos_from_readme('pytorch_awesome.md')\n",
    "inspect(repos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUDE_6Ge90Lm"
   },
   "source": [
    "Download zip files. For demonstration we only use 10 repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "sL0AOQZovkCU",
    "outputId": "5f17e2e1-2b7c-48d5-b02f-e2ad76452104"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'repos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e32881dc232b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrepos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmonit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Download {len(repos)} repos\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m#  Download the repository zip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mzip_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Extract the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'repos' is not defined"
     ]
    }
   ],
   "source": [
    "repos = repos[:10]\n",
    "for i, r in monit.enum(f\"Download {len(repos)} repos\", repos):\n",
    "    #  Download the repository zip\n",
    "    zip_file = create_dataset.download_repo(r[0], r[1], i)\n",
    "    # Extract the zip file\n",
    "    extracted = create_dataset.extract_zip(zip_file)\n",
    "    # Remove non .py files\n",
    "    create_dataset.remove_files(extracted, {'.py'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of python files across all repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "FqVq3acy4LYg",
    "outputId": "f09061d6-f839-4558-fa3c-84744f0f1b6b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\"><span style=\"color: #60C6C8\">   0: </span><strong>PythonFile(relative_path='examples/simultaneous_translation/eval/client.py', pro</strong><span style=\"color: #DDB62B\"> ...</span>\n",
       "<span style=\"color: #60C6C8\">   1: </span><strong>PythonFile(relative_path='examples/wav2vec/libri_labels.py', project='facebookre</strong><span style=\"color: #DDB62B\"> ...</span>\n",
       "<span style=\"color: #60C6C8\">   2: </span><strong>PythonFile(relative_path='onmt/bin/average_models.py', project='OpenNMT_OpenNMT-</strong><span style=\"color: #DDB62B\"> ...</span>\n",
       "<span style=\"color: #60C6C8\">   3: </span><strong>PythonFile(relative_path='fairseq/tasks/__init__.py', project='facebookresearch_</strong><span style=\"color: #DDB62B\"> ...</span>\n",
       "<span style=\"color: #60C6C8\">   4: </span><strong>PythonFile(relative_path='fairseq/modules/quantization/scalar/modules/qlinear.py</strong><span style=\"color: #DDB62B\"> ...</span>\n",
       "<span style=\"color: #60C6C8\">   5: </span><strong>PythonFile(relative_path='data_utils/loaders.py', project='NVIDIA_sentiment-disc</strong><span style=\"color: #DDB62B\"> ...</span>\n",
       "<span style=\"color: #60C6C8\">   6: </span><strong>PythonFile(relative_path='fairseq/data/audio/feature_transforms/utterance_cmvn.p</strong><span style=\"color: #DDB62B\"> ...</span>\n",
       "<span style=\"color: #60C6C8\">   7: </span><strong>PythonFile(relative_path='fairseq/logging/progress_bar.py', project='facebookres</strong><span style=\"color: #DDB62B\"> ...</span>\n",
       "<span style=\"color: #60C6C8\">   8: </span><strong>PythonFile(relative_path='models/classification_attention_rnn.py', project='Sand</strong><span style=\"color: #DDB62B\"> ...</span>\n",
       "<span style=\"color: #60C6C8\">   9: </span><strong>PythonFile(relative_path='seq2seq/trainer/supervised_trainer.py', project='IBM_p</strong><span style=\"color: #DDB62B\"> ...</span>\n",
       "<span style=\"color: #208FFB\">...</span>\n",
       "Total <span style=\"color: #208FFB\">1024</span> item(s)</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "source_files = create_dataset.get_python_files()\n",
    "np.random.shuffle(source_files)\n",
    "inspect(source_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the files into training and validation and merge them into `train.py` and `valid.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "AURmsHE24XbX",
    "outputId": "4b3b9be7-4728-4f90-8889-0b8c7d8e832d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\">Write train.py<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t1,071.21ms</span>\n",
       "Write valid.py<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t357.69ms</span>\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_valid_split = int(len(source_files) * 0.9)\n",
    "create_dataset.concat_and_save(lab.get_data_path() / 'train.py', source_files[:train_valid_split])\n",
    "create_dataset.concat_and_save(lab.get_data_path() / 'valid.py', source_files[train_valid_split:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Training script is defined in [`train.py`](https://github.com/lab-ml/python_autocomplete/blob/master/python_autocomplete/train.py).\n",
    "We import the `Configs` class from it and create a new experiment with\n",
    "custom configurations.\n",
    "You can experiment with changing these configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "qFGGXj7H33sX"
   },
   "outputs": [],
   "source": [
    "from python_autocomplete.train import Configs\n",
    "from labml import experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize `Configs` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "D4vMS55SQkFd"
   },
   "outputs": [],
   "source": [
    "conf = Configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.create(name=\"python_autocomplete\",\n",
    "                  comment='Colab demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0NV9wo4Q6mb"
   },
   "source": [
    "Set configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary for custom configurations. You can see the options available for configurations from these configurations of a [previous training I ran](https://web.lab-ml.com/configs?uuid=39b03a1e454011ebbaff2b26e3148b3d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_conf = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try a `transformer_model`, you can use `lstm_model` if you want to try a LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_conf['model'] = 'transformer_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of layers for the model, we set `2` for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_conf['n_layers'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size for training, you will have to reduce this if you use a larger model due to GPU memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_conf['batch_size'] = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of epochs. I usually set a high number on stop the training when the validation loss stops improving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_conf['epochs'] = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [Noam optimizer](https://lab-ml.com/labml_nn/optimizers/noam.html) (one used in original Transformers paper). You can also use something like `AdamW` (Adam with warmup). Transformer training usually needs a warmup session where the learning rate is kept low during initial training steps. \n",
    "\n",
    "Note that the learning rate is `1.0`, the actual learning rate will be $<10^{-3}$ because [Noam optimizer](https://lab-ml.com/labml_nn/optimizers/noam.html) factors the learning rate by $\\frac{1}{\\sqrt{d_{model}}}$ where $d_{model}$ is the number of dimensions in the transformer feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_conf['optimizer.optimizer'] = 'Noam'\n",
    "custom_conf['optimizer.learning_rate'] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of characters in a sample. This defaults to `512`, but we specify it here to make it easier to change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_conf['seq_len'] = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training switches between training and validation within an epoch, so that we get a the validation loss (for a fraction of vlaidation data) more frequently. This is especially useful when an epoch take a lot longer to train. `inner_iterations` should be increased depending on how large the training dataset is (hence longer time per epoch). We set it at 5 since we are only training on $10$ repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_conf['inner_iterations'] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "EsV-pjvo6hbq",
    "outputId": "d67818e6-c441-4fb5-ac04-1099cfcde96a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\"></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment.configs(conf, custom_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add models for saving and loading. If you plan to stop and continue the training you should include the optimizer also for saving.\n",
    "\n",
    "*Accessing `conf.model` loads the model and the dataset to calculate the number of tokens (specific characters) (needed to initialize the model)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "JU-DcoCr39f3",
    "outputId": "7b51d668-c898-4ec8-d353-e328a59d7cd8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\">Prepare model...\n",
       "  Prepare n_tokens...\n",
       "    Prepare text...\n",
       "      Prepare tokenizer<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t6.41ms</span>\n",
       "      Load data<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t8.76ms</span>\n",
       "      Tokenize<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t145.58ms</span>\n",
       "      Build vocabulary<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t143.89ms</span>\n",
       "    Prepare text<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t317.92ms</span>\n",
       "  Prepare n_tokens<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t328.08ms</span>\n",
       "  Prepare device...\n",
       "    Prepare device_info<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t69.23ms</span>\n",
       "  Prepare device<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t72.23ms</span>\n",
       "Prepare model<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t10,931.10ms</span>\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment.add_pytorch_models({'model': conf.model})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the experiment and run it. You can optionally load and continue from a previously saved run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "9C_nSTOnQveY",
    "outputId": "c0535ac6-44aa-4123-b465-1eee014add5a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\">\n",
       "<strong><span style=\"text-decoration: underline\">python_autocomplete</span></strong>: <span style=\"color: #208FFB\">c2c051c643fc11ebb7420242ac1c0002</span>\n",
       "\t<strong><span style=\"color: #DDB62B\">lstm model</span></strong>\n",
       "\t[dirty]: <strong><span style=\"color: #DDB62B\">\"\"</span></strong>\n",
       "Initialize...\n",
       "  Prepare mode<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t7.00ms</span>\n",
       "Initialize<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t60.84ms</span>\n",
       "Prepare validator...\n",
       "  Prepare valid_loader<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t119.45ms</span>\n",
       "Prepare validator<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t192.55ms</span>\n",
       "Prepare trainer...\n",
       "<span style=\"color: #C5C1B4\"></span>\n",
       "<span style=\"color: #C5C1B4\">--------------------------------------------------</span><span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>\n",
       "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\">LABML WARNING</span></strong></span>\n",
       "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>LabML App Warning: <span style=\"color: #60C6C8\">empty_token: </span><strong>Please create a valid token at https://web.lab-ml.com.</strong>\n",
       "<strong>Click on the experiment link to monitor the experiment and add it to your experiments list.</strong><span style=\"color: #C5C1B4\"></span>\n",
       "<span style=\"color: #C5C1B4\">--------------------------------------------------</span>\n",
       "<span style=\"color: #208FFB\">Monitor experiment at </span><a href='https://web.lab-ml.com/run?run_uuid=c2c051c643fc11ebb7420242ac1c0002' target='blank'>https://web.lab-ml.com/run?run_uuid=c2c051c643fc11ebb7420242ac1c0002</a>\n",
       "  Prepare train_loader<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t643.04ms</span>\n",
       "Prepare trainer<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t672.73ms</span>\n",
       "Prepare training_loop...\n",
       "  Prepare loop_count<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t84.22ms</span>\n",
       "Prepare training_loop<span style=\"color: #00A250\">...[DONE]</span><span style=\"color: #208FFB\">\t255.67ms</span>\n",
       "<span style=\"color: #C5C1B4\">def train(</span><strong>P</strong><strong>P</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong><strong>~</strong>\n",
       "<span style=\"color: #C5C1B4\">def train(</span><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong><strong>e</strong>\n",
       "<span style=\"color: #C5C1B4\">def train(</span><strong>e</strong><strong>t</strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong><strong> </strong>\n",
       "<strong><span style=\"color: #DDB62B\">  43,008:  </span></strong>Sample:<span style=\"color: #C5C1B4\"> 100%</span><span style=\"color: #208FFB\">   436ms  </span>Train:<span style=\"color: #C5C1B4\">  48%</span><span style=\"color: #208FFB\"> 249,351ms  </span>Valid:<span style=\"color: #C5C1B4\">  36%</span><span style=\"color: #208FFB\"> 4,423ms  </span> loss.train: <strong> 2.65108</strong> loss.valid: <span style=\"color: #C5C1B4\"> 2.79846</span>  <span style=\"color: #208FFB\">254,211ms</span><span style=\"color: #D160C4\">  0:02m/  2:13m  </span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# experiment.load('d5ba7f56d88911eaa6629b54a83956dc')\n",
    "with experiment.start():\n",
    "    conf.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PUJL3TYxQzro"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "labml autocomplete",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
